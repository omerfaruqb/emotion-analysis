{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model has been converted to ONNX and saved at ../models/resnet18_v2.onnx\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.onnx\n",
    "from torchvision import models\n",
    "\n",
    "# Load the model\n",
    "model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "model.fc = torch.nn.Linear(model.fc.in_features, 7)\n",
    "state_dict = torch.load('..\\\\models\\\\resnet18_weighted.pth', weights_only=True)\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "# Define a dummy input tensor with the same shape as the input data\n",
    "dummy_input = torch.randn(1, 3, 100, 100)  # Adjust the shape according to your model's input\n",
    "\n",
    "# Export the model to ONNX format\n",
    "onnx_path = \"../models/resnet18_v2.onnx\"\n",
    "torch.onnx.export(model, dummy_input, onnx_path, \n",
    "                  export_params=True, opset_version=10, \n",
    "                  do_constant_folding=True, \n",
    "                  input_names=['input'], output_names=['output'])\n",
    "\n",
    "print(f\"Model has been converted to ONNX and saved at {onnx_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the ONNX model\n",
    "import onnx\n",
    "onnx_path = \"../models/resnet18.onnx\"\n",
    "model = onnx.load(onnx_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 [[0.02793481 0.00141386 0.00485399 0.03123341 0.05615688 0.00480732\n",
      "  0.8735997 ]]\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "img = Image.open(r'C:\\Users\\Faruq\\Desktop\\DataScience\\Projects\\emotion-analysis\\rafdb_data\\test\\6\\test_0228_aligned.jpg')\n",
    "img = img.resize((100, 100))\n",
    "img = np.array(img)\n",
    "img = img / 255.0\n",
    "img = img * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])\n",
    "img = img.transpose((2, 0, 1))\n",
    "img = np.expand_dims(img, axis=0)\n",
    "img = img.astype(np.float32)\n",
    "img.shape\n",
    "\n",
    "# Inference\n",
    "model = onnx.load(onnx_path)\n",
    "onnx.checker.check_model(model)\n",
    "import onnxruntime as rt\n",
    "\n",
    "ort_session = rt.InferenceSession(onnx_path)\n",
    "input_name = ort_session.get_inputs()[0].name\n",
    "ort_inputs = {input_name: img}\n",
    "outputs = ort_session.run(None, ort_inputs)\n",
    "\n",
    "probs = np.exp(outputs[0]) / np.sum(np.exp(outputs[0]))\n",
    "print(np.argmax(probs), probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = {'0': 'Surprise', '1': 'Fear', '2': 'Disgust', '3': 'Happy', '4': 'Sadness', '5': 'Anger', '6': 'Neutral'}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
